{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "nlp_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cylveGhPTil",
        "colab_type": "text"
      },
      "source": [
        "# DO NOT RERUN TAKES A LONG TIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbcFbqtzEqjp",
        "colab_type": "code",
        "outputId": "49c148e5-e868-4f5d-f3c8-55fccf4a0e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZY-frVAEwjD",
        "colab_type": "code",
        "outputId": "e124989d-970c-41f8-d048-92c85caf801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8mvryF2EeUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL0sCs17EeUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"drive/My Drive/cs109bproject/df_project_unique.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CadzEO4rEeU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(labels=['Unnamed: 0'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL6NS_JrEeUs",
        "colab_type": "text"
      },
      "source": [
        "# NLP Prediction of Budget and Schedule Changes \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEgOzVN-xE7u",
        "colab_type": "text"
      },
      "source": [
        "## a) Preparing Our Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_gaY_k1xNiL",
        "colab_type": "text"
      },
      "source": [
        "### Creation of the input X\n",
        "The input will be the concatenation of \"Project Name\",\"Category\",\"Borough\" and \"Description\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXMUsYY9EeU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my columns \n",
        "#'Total Budget Changes (Scaled to Original)'\n",
        "#'Total Schedule Changes (Scaled to Original)'\n",
        "df['Total Budget Changes (Scaled to Original)']=df['Total Budget Changes (Scaled to Original)'].fillna(0)\n",
        "df['Total Schedule Changes (Scaled to Original)']=df['Total Schedule Changes (Scaled to Original)'].fillna(0)\n",
        "\n",
        "df[\"Project Name\"]=df[\"Project Name\"].fillna('')\n",
        "df[\"Category\"]=df[\"Category\"].fillna('')\n",
        "df[\"Borough\"]=df[\"Borough\"].fillna('')\n",
        "df[\"Description\"]=df[\"Description\"].fillna('')\n",
        "\n",
        "df[\"text\"]=df[\"Project Name\"] + ', ' + df[\"Category\"] + ' in ' + df[\"Borough\"] + ': ' + df[\"Description\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5nI-kd9VEeU8",
        "colab_type": "code",
        "outputId": "e1acc59b-3469-4ac7-cf7f-cd282138ad7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df[\"text\"].iloc[np.random.randint(100)]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Brunswick Avenue Sewers and Watermain Construction, Sewers in Queens: Storm and Sanitary sewers in Brunswick Ave. b/t Doughty Blvd. and Nameoke Ave, etc.Queens.  The storm sewer are needed to alleyiate ponding condition in the area and provide resisdents with adequate storm sewer.  New sanitary sewers have been included to service adjoining houses. There is water main work assciated with server project.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1EZNUfcxqId",
        "colab_type": "text"
      },
      "source": [
        "### Creation of the labels Y\n",
        "\n",
        "For Budget And Schedule changes, we will divide into 2 Categories :\n",
        "- \"Not Late/Reasonably Late\" or \"Too Late\" : threshold set at 10%\n",
        "- \"Under Budget/Reasonably Over Budget\" or \"Over Budget\" : threshold set at 5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMsAzJqqEeVD",
        "colab_type": "code",
        "outputId": "a415feaf-4a0c-40f4-f6b4-3767be974aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "try :\n",
        "    df=df.drop(index=df[df[\"Total Budget Changes (Scaled to Original)\"]==np.inf].index)\n",
        "except :\n",
        "    print(\"Already done\")\n",
        "df[[\"Total Budget Changes (Scaled to Original)\",'Total Schedule Changes (Scaled to Original)']].describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total Budget Changes (Scaled to Original)</th>\n",
              "      <th>Total Schedule Changes (Scaled to Original)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>313.000000</td>\n",
              "      <td>313.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.621585</td>\n",
              "      <td>-0.116365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.365947</td>\n",
              "      <td>4.430884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-64.913043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.000645</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.026811</td>\n",
              "      <td>0.109594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.324352</td>\n",
              "      <td>0.378151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>59.592113</td>\n",
              "      <td>2.886424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Total Budget Changes (Scaled to Original)  Total Schedule Changes (Scaled to Original)\n",
              "count                                 313.000000                                   313.000000\n",
              "mean                                    0.621585                                    -0.116365\n",
              "std                                     4.365947                                     4.430884\n",
              "min                                    -1.000000                                   -64.913043\n",
              "25%                                    -0.000645                                     0.000000\n",
              "50%                                     0.026811                                     0.109594\n",
              "75%                                     0.324352                                     0.378151\n",
              "max                                    59.592113                                     2.886424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aX8LIyZgEeVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_label(x,level):\n",
        "    if x<=level:\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "df['labels_Schedule']=df[\"Total Schedule Changes (Scaled to Original)\"].map(lambda x : create_label(x,0.1))\n",
        "df['labels_Budget']=df[\"Total Budget Changes (Scaled to Original)\"].map(lambda x : create_label(x,0.05))\n",
        "#5 per cent Budget change is acceptable\n",
        "#10 per cent Schedule change could be acceptable \n",
        "#arbitrary thresholds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vSX7I8kEeVH",
        "colab_type": "text"
      },
      "source": [
        "## b) Preprocessing Dataset Using Tokenisers\n",
        "\n",
        "Set max_length to 240 as we see that the input never has an length of more then 240. We are constrained by the max_length of Bert that is 520 and we do not want too many long input for no reason."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7-Jf1HeEeVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification,BertTokenizer\n",
        "#from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "#12-layer, 768-hidden, 12-heads, 110M parameters.\n",
        "#Trained on lower-cased English text."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfI9Zbd7H_mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 240\n",
        "batch_size=6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_vX7UVfzcGb",
        "colab_type": "text"
      },
      "source": [
        "### Convert text to Token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT05cQBEGgdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_feature(text):\n",
        "  \n",
        "  # combine step for tokenization, WordPiece vector mapping and will add also special tokens and truncate reviews longer than our max length\n",
        "  \n",
        "  return tokenizer.encode_plus(text, \n",
        "                add_special_tokens = True, # add [CLS], [SEP]\n",
        "                max_length = max_length, # max length of the text that can go to BERT\n",
        "                pad_to_max_length = True, # add [PAD] tokens\n",
        "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "  \n",
        "  # map to the expected input to TFBertForSequenceClassification, see here \n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks}, label\n",
        "\n",
        "def encode_text(df,column_name='labels_Schedule', limit=-1):\n",
        "\n",
        "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  label_list = []\n",
        "\n",
        "  if (limit > 0):\n",
        "      ds = ds.take(limit)\n",
        "\n",
        "  for text, label in zip(df.text, df[column_name]):\n",
        "\n",
        "    bert_input = convert_text_to_feature(text)\n",
        "\n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "    label_list.append([label])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXF8OB17zn8v",
        "colab_type": "text"
      },
      "source": [
        "### Split Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axTp94p8HfYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train,df_test=train_test_split(df,test_size=0.1)\n",
        "\n",
        "# train dataset\n",
        "ds_train_encoded = encode_text(df_train,column_name='labels_Schedule').shuffle(10000).batch(batch_size)\n",
        "\n",
        "# test dataset\n",
        "ds_test_encoded = encode_text(df_test,column_name='labels_Schedule').shuffle(10000).batch(batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twbsgTFdzY1q",
        "colab_type": "text"
      },
      "source": [
        "## c) Fine-Tuning of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUdBCvFzIWph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize model and weights\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
        "number_of_epochs = 3\n",
        "\n",
        "\n",
        "# classifier Adam recommended\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqAljnzRIW1u",
        "colab_type": "code",
        "outputId": "6d1f2b1a-739a-4bad-da8a-a8b8d57a2cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded) #with2e-5"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "47/47 [==============================] - 757s 16s/step - loss: 0.6613 - accuracy: 0.5872 - val_loss: 0.6036 - val_accuracy: 0.6875\n",
            "Epoch 2/3\n",
            "47/47 [==============================] - 742s 16s/step - loss: 0.5529 - accuracy: 0.7367 - val_loss: 0.5638 - val_accuracy: 0.6875\n",
            "Epoch 3/3\n",
            "47/47 [==============================] - 748s 16s/step - loss: 0.4890 - accuracy: 0.7865 - val_loss: 0.5007 - val_accuracy: 0.7812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmC4lUI6R7bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "fd047107-468b-4b6f-92f0-025141e9fdcb"
      },
      "source": [
        "model.save('Bert_labels_Schedule')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: Bert_labels_Schedule/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3seyXKemS-UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow import keras\n",
        "#model1 = keras.models.load_model('Bert_labels_Schedule')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBnLimXQUoOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r Bert_labels_Schedule.zip Bert_labels_Schedule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i60ZsEqR0T4i",
        "colab_type": "text"
      },
      "source": [
        "## d) Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSQ8A3JIXao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adb3a33f-9ca4-4995-be6a-4a579b54e5c2"
      },
      "source": [
        "df_encoded=encode_text(df,column_name='labels_Schedule').batch(batch_size)\n",
        "preds=model.predict(df_encoded,verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53/53 [==============================] - 243s 5s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N1f7-lmIXZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    score=[]\n",
        "    for k in x :\n",
        "      e_k = np.exp(k - np.max(k))\n",
        "      e_k=e_k / e_k.sum()\n",
        "      score.append(e_k.tolist())\n",
        "    return np.array(score)\n",
        "\n",
        "scores=softmax(preds[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HADZUPVOIXYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores=scores.argmax(axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvITZ4K4IXTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EctH_rWIXSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "36b1e493-01e9-40d5-d95c-51acc09c8989"
      },
      "source": [
        "confusion_matrix(df['labels_Schedule'].values,scores)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[127,  26],\n",
              "       [ 20, 140]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dByxY0JTIXIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHfHKqp2IXHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}